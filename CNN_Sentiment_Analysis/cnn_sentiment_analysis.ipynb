{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "cnn_sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TFX7wSAmg9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b43a53b-eaf0-43b0-9262-f5ce1de72678"
      },
      "source": [
        "!pip install pyvi\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from string import digits\n",
        "from collections import Counter\n",
        "from pyvi import ViTokenizer\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from keras.utils.np_utils import to_categorical\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 22.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.22.2.post1)\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.0.1)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.62.3)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.7 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "a7lMy03omg93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "f4c1e509-665a-437d-a11f-0a8b55f36fb8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_train = pd.read_csv(\"/content/drive/MyDrive/Learning/Natural Language Processing/Exercises/cnn/vlsp_sentiment_train.csv\", sep='\\t')\n",
        "data_train.columns =['Class', 'Data']\n",
        "print(\"Trainning sample\\n\")\n",
        "data_train.head()\n",
        "data_test = pd.read_csv(\"/content/drive/MyDrive/Learning/Natural Language Processing/Exercises/cnn/vlsp_sentiment_test.csv\", sep='\\t')\n",
        "data_test.columns =['Class', 'Data']\n",
        "print(\"Test sample\\n\")\n",
        "data_test.tail()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Trainning sample\n",
            "\n",
            "Test sample\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>Data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1045</th>\n",
              "      <td>0</td>\n",
              "      <td>30 củ à :)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1046</th>\n",
              "      <td>0</td>\n",
              "      <td>Apple bán dc thi samsung cũng lời nhiêu. Ng...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1047</th>\n",
              "      <td>0</td>\n",
              "      <td>có thể giúp android vượt trội so với ios chớ c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048</th>\n",
              "      <td>0</td>\n",
              "      <td>Mẹ mình từng sang Đài Loan và có mua 1 cái iph...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1049</th>\n",
              "      <td>0</td>\n",
              "      <td>Tùng Minh Nguyễn điện thoại của vk bị như này ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Class                                               Data\n",
              "1045      0                                         30 củ à :)\n",
              "1046      0  Apple bán dc thi samsung cũng lời nhiêu. Ng...\n",
              "1047      0  có thể giúp android vượt trội so với ios chớ c...\n",
              "1048      0  Mẹ mình từng sang Đài Loan và có mua 1 cái iph...\n",
              "1049      0  Tùng Minh Nguyễn điện thoại của vk bị như này ..."
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HR1jAzImg94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35123f92-be91-4a09-ca15-6825b6b515d9"
      },
      "source": [
        "print(data_train.shape)\n",
        "print(data_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5100, 2)\n",
            "(1050, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvrbwPfZmg95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f330eded-0ae1-4e6f-a7c5-ebf4442b5a0a"
      },
      "source": [
        "labels = data_train.iloc[:, 0].values\n",
        "reviews = data_train.iloc[:, 1].values\n",
        "print(reviews)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mình đã dùng anywhere thế hệ đầu, quả là đầy thất vọng, hiện tại đang vứt xó. Giá thì đắt, ngốn pin như ăn gỏi, nặng'\n",
            " 'Quan tâm nhất là độ trễ có cao không, dùng thi thoảng nó cứ trễ bực mình, đấy mới chỉ là dùng văn phòng chứ game thì chắc là ném đi từ lâu. Không biết con này có độ trễ không nhỉ. Dùng nhiều loại nhưng vẫn kết nhất con chuột sứ mitsumi, gọn bấm nảy tốt'\n",
            " 'dag xài con cùi bắp 98k....pin trâu, mỗi tội đánh liên minh ức chế đập hết 2 con'\n",
            " ...\n",
            " 'Dùng oppo mà bộ nhớ 4gb thì k chơi games đc đâu.hệ thống đã chiếm 2,2gb rồi.chuyển sag thẻ sd thì cứ như k sag vậy :('\n",
            " 'Sao tui thích xài hàng oppo mà lựa toàn mấy đứa tui ghét quảng cáo vậy trời'\n",
            " 'mới mở hộp ,oy mở vào camera mà đã có ảnh chụp trc đó ở góc màn hình oy.... thấy sai sai']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HlbVeHimg95"
      },
      "source": [
        "encoded_labels = []\n",
        "\n",
        "for label in labels:\n",
        "    if label == -1:\n",
        "        encoded_labels.append([1,0,0])\n",
        "    elif label == 0:\n",
        "        encoded_labels.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels.append([0,0,1])\n",
        "\n",
        "encoded_labels = np.array(encoded_labels)  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm4OCwxXmg96"
      },
      "source": [
        "reviews_processed = []\n",
        "unlabeled_processed = [] \n",
        "for review in reviews:\n",
        "    review_cool_one = ''.join([char for char in review if char not in digits])\n",
        "    reviews_processed.append(review_cool_one)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW2OZgkgmg97"
      },
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews = []\n",
        "all_words = []\n",
        "for review in reviews_processed:\n",
        "    review = ViTokenizer.tokenize(review.lower())\n",
        "    word_reviews.append(review.split())\n",
        "   "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTb0MeDRmg98"
      },
      "source": [
        "EMBEDDING_DIM = 400 # how big is each word vector\n",
        "MAX_VOCAB_SIZE = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW-7mKtWmg9-"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BHpPSLTmg9_"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(word_reviews)\n",
        "sequences_train = tokenizer.texts_to_sequences(word_reviews)\n",
        "word_index = tokenizer.word_index\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlV3M2dimg9_"
      },
      "source": [
        "data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = encoded_labels"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dl9VZ3Rmg-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5598942e-41a9-4957-880e-8e0ef7f903a9"
      },
      "source": [
        "print('Shape of X train and X validation tensor:',data.shape)\n",
        "print('Shape of label train and validation tensor:', labels.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (5100, 300)\n",
            "Shape of label train and validation tensor: (5100, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KKSjJdJmg-A"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Learning/Natural Language Processing/Exercises/cnn/vi-model-CBOW.bin', binary=True)\n",
        "\n",
        "\n",
        "vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n",
        "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i>=MAX_VOCAB_SIZE:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = word_vectors[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "\n",
        "del(word_vectors)\n",
        "\n",
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(vocabulary_size,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njBANdn5mg-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a413be-cdc8-48d2-8677-b2c5efef37f5"
      },
      "source": [
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
        "from keras.layers.core import Reshape, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "sequence_length = data.shape[1]\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 100\n",
        "drop = 0.5\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
        "\n",
        "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "\n",
        "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
        "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
        "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
        "\n",
        "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
        "flatten = Flatten()(merged_tensor)\n",
        "reshape = Reshape((3*num_filters,))(flatten)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "\n",
        "\n",
        "# this creates a model that includes\n",
        "model = Model(inputs, output)\n",
        "\n",
        "\n",
        "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "#define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 300)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 300, 400)     3167600     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 300, 400, 1)  0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 298, 1, 100)  120100      reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 297, 1, 100)  160100      reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 296, 1, 100)  200100      reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 1, 1, 100)    0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 100)    0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3, 1, 100)    0           max_pooling2d[0][0]              \n",
            "                                                                 max_pooling2d_1[0][0]            \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 300)          0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 300)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 3)            903         dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,648,803\n",
            "Trainable params: 3,648,803\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn0dBlzjmg-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c340cd96-0f99-4793-ff2c-f2df624ef78c"
      },
      "source": [
        "model.fit(data, labels, validation_split=0.2,\n",
        "          epochs =5, batch_size=256,callbacks=callbacks_list, shuffle=True, verbose=2)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "16/16 - 103s - loss: 0.8174 - accuracy: 0.5412 - val_loss: 1.3891 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "16/16 - 101s - loss: 0.6355 - accuracy: 0.6745 - val_loss: 1.1447 - val_accuracy: 0.0422\n",
            "Epoch 3/5\n",
            "16/16 - 101s - loss: 0.5617 - accuracy: 0.7353 - val_loss: 1.0603 - val_accuracy: 0.1098\n",
            "Epoch 4/5\n",
            "16/16 - 101s - loss: 0.5144 - accuracy: 0.7801 - val_loss: 1.1838 - val_accuracy: 0.0559\n",
            "Epoch 5/5\n",
            "16/16 - 101s - loss: 0.4851 - accuracy: 0.8083 - val_loss: 1.1668 - val_accuracy: 0.0784\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff8c9bc4390>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XoN2UOamg-D"
      },
      "source": [
        "labels_test = data_test.iloc[:, 0].values\n",
        "reviews_test = data_test.iloc[:, 1].values"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwiYb3Ohmg-E"
      },
      "source": [
        "encoded_labels_test = []\n",
        "\n",
        "for label_test in labels_test:\n",
        "    if label_test == -1:\n",
        "        encoded_labels_test.append([1,0,0])\n",
        "    elif label_test == 0:\n",
        "        encoded_labels_test.append([0,1,0])\n",
        "    else:\n",
        "        encoded_labels_test.append([0,0,1])\n",
        "\n",
        "encoded_labels_test = np.array(encoded_labels_test)  "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E08tBw9img-E"
      },
      "source": [
        "reviews_processed_test = []\n",
        "unlabeled_processed_test = [] \n",
        "for review_test in reviews_test:\n",
        "    review_cool_one = ''.join([char for char in review_test if char not in digits])\n",
        "    reviews_processed_test.append(review_cool_one)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwgI9Xywmg-E"
      },
      "source": [
        "#Use PyVi for Vietnamese word tokenizer\n",
        "word_reviews_test = []\n",
        "all_words = []\n",
        "for review_test in reviews_processed_test:\n",
        "    review_test = ViTokenizer.tokenize(review_test.lower())\n",
        "    word_reviews_test.append(review_test.split())"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p02GxCh6mg-F"
      },
      "source": [
        "sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n",
        "data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels_test = encoded_labels_test"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAqUMGInmg-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f68cd2-9fde-4ba6-9ab7-bbd3e4af2ccd"
      },
      "source": [
        "print('Shape of X train and X validation tensor:',data_test.shape)\n",
        "print('Shape of label train and validation tensor:', labels_test.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X train and X validation tensor: (1050, 300)\n",
            "Shape of label train and validation tensor: (1050, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKclttiOmg-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19f413d-c2ab-4b79-bba7-e24ea7b91bff"
      },
      "source": [
        "score = model.evaluate(data_test, labels_test)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 5s 166ms/step - loss: 0.6635 - accuracy: 0.6124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r31_uxxgmg-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f125f09-8b20-44bc-c701-85fefb79b4bd"
      },
      "source": [
        "print(\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100))\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 66.35%\n",
            "accuracy: 61.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGwnvKMKQca8"
      },
      "source": [
        "Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8O3z4IFmg-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b0ad154-d761-48bc-b947-ec5476479bb5"
      },
      "source": [
        "test = \"Hãy yêu nhau đi\"\n",
        "reviews_processed_test = []\n",
        "\n",
        "review_not_contain_digit = ''.join([char for char in test if char not in digits])\n",
        "reviews_processed_test.append(review_not_contain_digit)\n",
        "\n",
        "word_reviews_test = []\n",
        "\n",
        "for review in reviews_processed_test:\n",
        "    review = ViTokenizer.tokenize(review.lower())\n",
        "    word_reviews_test.append(review.split())\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(word_reviews_test)\n",
        "sequences_train = tokenizer.texts_to_sequences(word_reviews_test)\n",
        "\n",
        "sampleToPredit = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "class_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "pre = model.predict(sampleToPredit)\n",
        "print(class_names[np.argmax(pre)])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGcigViqQf3b",
        "outputId": "dc540d68-1be3-4eb2-ca21-d63d77f721f8"
      },
      "source": [
        "test = \"Dấu tình sầu\"\n",
        "reviews_processed_test = []\n",
        "\n",
        "review_not_contain_digit = ''.join([char for char in test if char not in digits])\n",
        "reviews_processed_test.append(review_not_contain_digit)\n",
        "\n",
        "word_reviews_test = []\n",
        "\n",
        "for review in reviews_processed_test:\n",
        "    review = ViTokenizer.tokenize(review.lower())\n",
        "    word_reviews_test.append(review.split())\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(word_reviews_test)\n",
        "sequences_train = tokenizer.texts_to_sequences(word_reviews_test)\n",
        "\n",
        "sampleToPredit = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "class_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "pre = model.predict(sampleToPredit)\n",
        "print(class_names[np.argmax(pre)])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    }
  ]
}