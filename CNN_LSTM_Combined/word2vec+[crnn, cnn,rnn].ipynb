{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"word2vec+[crnn, cnn,rnn].ipynb","provenance":[{"file_id":"1rFZZf9ECknkLNDv8so_kQNPhqain0RqJ","timestamp":1616587562665}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EFJo8MFr1Icj"},"source":["#  Install and Import required Libs\n","\n"]},{"cell_type":"code","metadata":{"id":"FK6Gp9sKxM3U"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRIs1xe4xp4Q"},"source":["!pip install pyvi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGnLGj-vwU9t"},"source":["import tensorflow as tf\n","import pandas as pd \n","import numpy as np\n","from string import digits\n","from collections import Counter\n","from pyvi import ViTokenizer\n","from gensim.models.word2vec import Word2Vec\n","from keras.utils.np_utils import to_categorical\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ns4vsLP-1XHc"},"source":["# Load train and test dataset"]},{"cell_type":"code","metadata":{"id":"Z7z2szHRwU9z","scrolled":true},"source":["def load_data():\n","  data_train = pd.read_csv(\"/content/drive/MyDrive/Learning/Natural Language Processing/Exercises/Bai Tap Lon/vlsp_sentiment_train.csv\", sep='\\t')\n","  data_train.columns =['Class', 'Data']\n","  data_aug = pd.read_csv(\"/content/drive/MyDrive/Learning/Natural Language Processing/Exercises/Bai Tap Lon/augment_data/train_augment.csv\")\n","  data_train = pd.concat((data_train, data_aug))\n","\n","  data_test = pd.read_csv(\"/content/drive/MyDrive/Learning/Natural Language Processing/Exercises/Bai Tap Lon/vlsp_sentiment_test.csv\", sep='\\t')\n","  data_test.columns =['Class', 'Data'] \n","  return data_train, data_test\n","\n","data_train, data_test = load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D7U6TJVNwU90"},"source":["print(data_train.shape)\n","print(data_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"prV4XxvywU90"},"source":["labels = data_train.iloc[:, 0].values\n","reviews = data_train.iloc[:, 1].values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o2e7__La1cwQ"},"source":["# Preprocess Data"]},{"cell_type":"code","metadata":{"id":"hQZymxX0wU91"},"source":["def encode_labels(labels):\n","  result = []\n","  for label in labels:\n","      if label == -1:\n","          result.append([1,0,0])\n","      elif label == 0:\n","          result.append([0,1,0])\n","      else:\n","          result.append([0,0,1])\n","\n","  return np.array(result)  \n","\n","encoded_labels = encode_labels(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"34J9F70iwU91"},"source":["def remove_digits(reviews_input):\n","  result=[]\n","  for review in reviews_input:\n","      review_cool_one = ''.join([char for char in review if char not in digits])\n","      result.append(review_cool_one)\n","  return result\n","\n","reviews_processed = remove_digits(reviews)\n","unlabeled_processed = [] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-uevU-I5wU92"},"source":["def tokenize(reviews_processed_input):\n","  result = []\n","  for review in reviews_processed_input:\n","      review = ViTokenizer.tokenize(review.lower())\n","      result.append(review.split())\n","  return result\n","\n","word_reviews = tokenize(reviews_processed)\n","all_words = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VvZBk6kfwU93"},"source":["EMBEDDING_DIM = 400 # how big is each word vector\n","MAX_VOCAB_SIZE = 20000 # how many unique words to use (i.e num rows in embedding vector)\n","MAX_SEQUENCE_LENGTH = 300 # max number of words in a comment to use"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcAfN337wU93"},"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0rXI-fxLwU94"},"source":["tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n","tokenizer.fit_on_texts(word_reviews)\n","sequences_train = tokenizer.texts_to_sequences(word_reviews)\n","word_index = tokenizer.word_index\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6wAXC5HrwU94"},"source":["data = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n","labels = encoded_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9n7P5xlZwU94"},"source":["print('Shape of X train and X validation tensor:',data.shape)\n","print('Shape of label train and validation tensor:', labels.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ns3TJ1A1h-w"},"source":["# Build and model"]},{"cell_type":"code","metadata":{"id":"cFwY64D7wU95"},"source":["import gensim\n","from gensim.models import Word2Vec\n","from gensim.utils import simple_preprocess\n","\n","from gensim.models.keyedvectors import KeyedVectors\n","from keras.layers import Embedding\n","\n","\n","def build_embedding_layer(word_index):\n","\n","  word_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Learning/Natural Language Processing/Exercises/Bai Tap Lon/vi-model-CBOW.bin', binary=True)\n","\n","\n","  vocabulary_size=min(len(word_index)+1,MAX_VOCAB_SIZE)\n","  embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n","  for word, i in word_index.items():\n","      if i>=MAX_VOCAB_SIZE:\n","          continue\n","      try:\n","          embedding_vector = word_vectors[word]\n","          embedding_matrix[i] = embedding_vector\n","      except KeyError:\n","          embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n","\n","  del(word_vectors)\n","\n","\n","  return Embedding(vocabulary_size,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            trainable=True)\n","  \n","\n","\n","embedding_layer = build_embedding_layer(word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TF5jaU0-wU95"},"source":["from keras.layers import Dense, Input, GlobalMaxPooling1D\n","from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, Permute\n","from keras.models import Model\n","from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate,ZeroPadding2D\n","from keras.layers.core import Reshape, Flatten\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam\n","from keras.models import Model\n","from keras import regularizers\n","from keras.metrics import Precision\n","from keras.metrics import Recall\n","\n","filter_sizes = [3,4,5]\n","num_filters = 100\n","drop = 0.5\n","def build_model(sequence_length, embedding_layer): \n","  inputs = Input(shape=(sequence_length,))\n","  embedding = embedding_layer(inputs)\n","\n","  ################## LSTM ONLY ###############################\n","  # reshape = Reshape((sequence_length,EMBEDDING_DIM))(embedding)\n","\n","  ################# SINGLE LSTM ####################\n","  # lstm_0 = LSTM(512)(reshape)\n","\n","  # YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n","  # lstm_2 = LSTM(1024, return_sequences=True)(reshape)\n","  # lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n","  # lstm_0 = LSTM(256)(lstm_1)\n","\n","  ############################################################\n","\n","\n","  ################## CRNN ####################################\n","  reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n","  conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n","  conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n","  conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n","\n","  # This will pad all output conv_ layer features to the same size (sequence_length, num_filters)\n","  conv_0 = ZeroPadding2D(((0, 2), (0, 0)))(conv_0)\n","  conv_1 = ZeroPadding2D(((0, 3), (0, 0)))(conv_1)\n","  conv_2 = ZeroPadding2D(((0, 4), (0, 0)))(conv_2)\n","\n","  conv_0 = Reshape((-1, num_filters))(conv_0)\n","  conv_1 = Reshape((-1, num_filters))(conv_1)\n","  conv_2 = Reshape((-1, num_filters))(conv_2)\n","\n","  concat = concatenate([conv_0, conv_1, conv_2])\n","\n","  lstm_0 = LSTM(512)(concat)\n","\n","  # YOU WANNA ADD MORE LSTM LAYERS? UNCOMMENT THIS #\n","  lstm_2 = LSTM(1024, return_sequences=True)(concat)\n","  lstm_1 = LSTM(512, return_sequences=True)(lstm_2)\n","  lstm_0 = LSTM(256)(lstm_1)\n","\n","############################################################\n","\n","  dropout = Dropout(drop)(lstm_0)\n","  output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n","\n","# this creates a model that includes\n","  model = Model(inputs, output)\n","\n","\n","  adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n","  model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy', Precision(), Recall()])\n","  model.summary()\n","  return model\n","\n","model = build_model(data.shape[1], embedding_layer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_p851nTHQMwL"},"source":["Zero padding: them cac so 0 deu de du 300-"]},{"cell_type":"code","metadata":{"id":"sUZo87gZGYlz"},"source":["### IF YOU HAVE MODEL WEIGHT AND WANNA LOAD IT\n","#model.load_weights(\"lstm_only.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTJGHh2hwU96"},"source":["#define callbacks\n","early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1)\n","callbacks_list = [early_stopping]\n","\n","history = model.fit(data, labels, validation_split=0.2,\n","          epochs=100, batch_size=256, callbacks=callbacks_list, shuffle=True, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xrpJd93ROSfm"},"source":["#Plot data"]},{"cell_type":"code","metadata":{"id":"9kTTmOiZOVjn"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","def show_graph(history):\n","  pd.DataFrame(history.history).plot(figsize = (16, 10))\n","  plt.grid(True)\n","  plt.gca().set_ylim(0, 1)\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Score')\n","  plt.show()\n","\n","show_graph(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KFcpn3Tg1Bhw"},"source":["# Evaluate Model"]},{"cell_type":"code","metadata":{"id":"dDWU9oCBwU96"},"source":["labels_test = data_test.iloc[:, 0].values\n","reviews_test = data_test.iloc[:, 1].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f2zBNnGJwU96"},"source":["encoded_labels_test = encode_labels(labels_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArJFkP6fwU97"},"source":["reviews_processed_test = []\n","unlabeled_processed_test = [] \n","for review_test in reviews_test:\n","    review_cool_one = ''.join([char for char in review_test if char not in digits])\n","    reviews_processed_test.append(review_cool_one)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JV9bR1nlwU97"},"source":["#Use PyVi for Vietnamese word tokenizer\n","word_reviews_test = []\n","all_words = []\n","for review_test in reviews_processed_test:\n","    review_test = ViTokenizer.tokenize(review_test.lower())\n","    word_reviews_test.append(review_test.split())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gvf83C-QwU98"},"source":["sequences_test = tokenizer.texts_to_sequences(word_reviews_test)\n","data_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n","labels_test = encoded_labels_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Zubh3jfwU98"},"source":["print('Shape of X train and X validation tensor:',data_test.shape)\n","print('Shape of label train and validation tensor:', labels_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzDo7cKqwU98"},"source":["score = model.evaluate(data_test, labels_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cE6zEWGNwU99"},"source":["print(\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100))\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n","print(\"%s: %.2f%%\" % (model.metrics_names[2], score[2]*100))\n","print(\"%s: %.2f%%\" % (model.metrics_names[3], score[3]*100))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O_SlhL45wU99"},"source":["model.save_weights(\"lstm_only.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1vicgONI2HH-"},"source":["# Test model"]},{"cell_type":"markdown","metadata":{"id":"msnieyCO8Z-j"},"source":["## Review 5*"]},{"cell_type":"code","metadata":{"id":"EYEy7lCZ2J6X"},"source":["test = \"Áo hơi mỏng nhưng rất đẹp hình thêu các thứ rất đẹp còn 1 vài chỗ có chỉ thừa phần 2 túi áo nên làm kiểu zip chất lương hơn tí hoặc có thể k cần zip cũng đc phần bo chun ống tay và cổ áo cần cải thiện thêm. Nói chung với giá đc sale xuống và mình dùng voucher nữa nên như này mình cũng hài lòng rồi\"\n","\n","reviews_processed_test = []\n","\n","review_not_contain_digit = ''.join([char for char in test if char not in digits])\n","reviews_processed_test.append(review_not_contain_digit)\n","\n","word_reviews_test = tokenize(reviews_processed_test)\n","\n","\n","tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n","tokenizer.fit_on_texts(word_reviews_test)\n","sequences_train = tokenizer.texts_to_sequences(word_reviews_test)\n","\n","sampleToPredit = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","class_names = [\"Negative\", \"Neutral\", \"Positive\"]\n","pre = model.predict(sampleToPredit)\n","print(pre)\n","print(class_names[np.argmax(pre)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AXsmdAhx8kNg"},"source":["## Review 3*"]},{"cell_type":"code","metadata":{"id":"IYSTjoBl6KXX"},"source":["test = \"Áo mỏng hơn so với mình nghĩ.... Ko xứng đáng với giá 450k sz L mà như M vậy\"\n","\n","reviews_processed_test = []\n","\n","review_not_contain_digit = ''.join([char for char in test if char not in digits])\n","reviews_processed_test.append(review_not_contain_digit)\n","\n","\n","tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n","tokenizer.fit_on_texts(word_reviews_test)\n","sequences_train = tokenizer.texts_to_sequences(word_reviews_test)\n","\n","sampleToPredit = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","pre = model.predict(sampleToPredit)\n","print(pre)\n","print(class_names[np.argmax(pre)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-nfWAlla8sCv"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"_iatq02G8sFF"},"source":["## Review 1*"]},{"cell_type":"code","metadata":{"id":"g8DZ9dQh8v3h"},"source":["test = \"Như cái rẻ lau chân chán kinh khủng, mua phí tiền.mặc được chết liền\"\n","\n","reviews_processed_test = []\n","\n","review_not_contain_digit = ''.join([char for char in test if char not in digits])\n","reviews_processed_test.append(review_not_contain_digit)\n","\n","\n","tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n","tokenizer.fit_on_texts(word_reviews_test)\n","sequences_train = tokenizer.texts_to_sequences(word_reviews_test)\n","\n","sampleToPredit = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","pre = model.predict(sampleToPredit)\n","print(pre)\n","print(class_names[np.argmax(pre)])"],"execution_count":null,"outputs":[]}]}